# ğŸ§  VisionText AI â€” InternVL3 Chat Interface

**VisionText AI** is a multimodal AI assistant prototype powered by [InternVL3-1B](https://huggingface.co/OpenGVLab/InternVL3-1B). It lets users engage in image + text conversations via a clean Gradio interface, ideal for testing vision-language models locally.

> ğŸš§ This is an early prototype. Future phases will add FastAPI, NLP models, Docker support, and full API endpoints.

---

## ğŸŒŸ Features

- ğŸ–¼ï¸ Chat with images and text using InternVL3
- ğŸ—¨ï¸ Multi-turn conversation memory
- âš™ï¸ Modular configuration via `config.yml`
- ğŸ›ï¸ Choose from predefined system prompts (e.g., medical, default, reasoning)

---

## ğŸ“ Project Structure

visiontextai/
â”œâ”€â”€ configs/
â”‚ â”œâ”€â”€ config.yml # Device/model settings
â”‚ â””â”€â”€ prompts.py # Customizable prompt types
â”‚
â”œâ”€â”€ src/
â”‚ â””â”€â”€ internVl.py # Gradio + inference logic
â”‚
â”œâ”€â”€ main.py # Entry point to launch the app
â””â”€â”€ README.md # You're here!
---

## ğŸš€ Getting Started

**1. Clone the repository and set up the environment:**
git clone https://github.com/yousaf44malik/visiontextai.git
cd visiontextai
conda create -n visiontextai python=3.11 -y
conda activate visiontextai
**2. Install dependencies:**
pip install torch torchvision transformers gradio pillow numpy einops timm

**3. Configure the model:**

Edit `configs/config.yml` as needed:
model_path: OpenGVLab/InternVL3-1B # Use HuggingFace path or local folder
device: cuda # "cuda" for GPU, "cpu" for CPU


**4. Run the app:**

python main.py

After startup, open your browser to:

http://127.0.0.1:7860

---

## ğŸ§ª Example Usage

- Enter a prompt like:  
  *"Describe this image in detail."*
- Optionally upload an image and ask:  
  *"What animal is this?"*
- Use the dropdown to switch between system prompt types:  
  `default`, `medical`, `reasoning`, etc.

---

## ğŸ”œ Roadmap

- âœ… InternVL3 chat UI with image + text input
- ğŸ”œ FastAPI support
- ğŸ”œ LLaVA and BLIP-2 integrations
- ğŸ”œ Dockerized deployment
- ğŸ”œ NLP-only endpoints (`/process-nlp`)
- ğŸ”œ Vision-only endpoints (`/process-image`)

---

## ğŸ¤ Contributing

Pull requests, issues, and discussions are welcome!

---

## ğŸ“„ License

This project is under the MIT License. See `LICENSE` for details.

---

*Let me know if you'd like to include a screenshot, demo video link, or CI badge later on.*
